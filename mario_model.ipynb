{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import load_maps, load_power_ups, load_characters\n",
    "\n",
    "map_data = load_maps()\n",
    "power_ups = load_power_ups()\n",
    "characters = load_characters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other packages\n",
    "import pandas as pd\n",
    "\n",
    "# RFC predicts on non-linear relationships\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# n_estimators is # of decision trees to train;\n",
    "# min_sample_split is num of samples in leaf before node is split\n",
    "# higher num, less accuracy\n",
    "rf = RandomForestClassifier(n_estimators = 50, min_samples_split = 10, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add col for both players in race\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training set to first 900 rows\n",
    "train = results.iloc[1:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set to last 100 rows\n",
    "test = results.iloc[900:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set predictors: baseline, map performance, power ups etc\n",
    "predictors = [\"baseline_perf\",\"map_perf\",\"powerups\",\"times_hit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf is using given predictors to guess \"target\" whether player won\n",
    "rf.fit(train[predictors], train[\"win_stat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "preds = rf.predict(test[predictors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check overall accuracy (model's overall correctness)\n",
    "acc = accuracy_score(test[\"target\", preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check precision (model correctness when predicting specific category)\n",
    "combined = pd.DataFrame(dict(actual=test[\"target\"], predict=preds))\n",
    "pd.crosstab(index=combined[\"actual\"], columns=combined[\"prediction\"])\n",
    "precision_score(test[\"target\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve precision by computing rolling averages\n",
    "grouped_matches = results.groupby(\"player1\")\n",
    "\n",
    "# ex: peach AND bowser \n",
    "group = grouped_matches.get_group(\"Peach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rolling averages based on previous matches between two characters\n",
    "def rolling_averages(group, cols, new_cols):\n",
    "    group = group.sort_values(\"index\") # not sure if we'll use index\n",
    "    rolling_stats = group[cols].rolling(3, closed='left').mean()\n",
    "    group[new_cols] = rolling_stats\n",
    "    group.dropna(subset=new_cols)\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to compute rolling averages for\n",
    "cols = [\"baseline\",\"map_perf\",\"map\",\"powerups\",\"times_hit\"]\n",
    "new_cols = [f\"{c}_rolling\" for c in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added info about what happened before\n",
    "rolling_averages(group, cols, new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates df based on players in race\n",
    "matches_rolling = results.groupby(\"player1\").apply(lambda x: rolling_averages(x, cols, new_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_rolling = matches_rolling.droplevel(\"playing1\")\n",
    "matches_rolling.index - range(matches_rolling.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain model\n",
    "def make_predictions(data, predictors):\n",
    "    train = results.iloc[1:900]\n",
    "    test = results.iloc[900:1000]\n",
    "    rf.fit(train[predictors], train[\"target\"])\n",
    "    preds = rf.predict(test[predictors])\n",
    "    combined = pd.DataFrame(dict(actual=test[\"target\"],predicted=preds), index=test.index)\n",
    "    precision = precision_score(test[\"target\"], preds)\n",
    "    return combined, precision"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
